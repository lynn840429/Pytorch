{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Logistic Regression.ipynb","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"TiWdWDdPcKTY","colab_type":"text"},"source":["一般Linear Regression是解决回归问题，而 Logistic Regression是应用于分类问题。Logistic Regression除了可以解决二分类问题外，还可以解决多分类问题。"]},{"cell_type":"code","metadata":{"id":"ud9C3zJYYVnK","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"5ccd2b03-9b7e-4c83-9b3e-3363c9cfa8e8","executionInfo":{"status":"ok","timestamp":1574783728266,"user_tz":-480,"elapsed":41798,"user":{"displayName":"Lynn","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAA3iweeMBLyNHjfWr2cL84enkEcjNXptNrvoZJUw=s64","userId":"13191593999274530441"}}},"source":["import torch\n","import torch.nn as nn\n","import torchvision\n","import torchvision.transforms as transforms\n","\n","\n","# Hyper-parameters \n","input_size = 784\n","num_classes = 10\n","num_epochs = 10\n","batch_size = 100\n","learning_rate = 0.001\n","\n","# MNIST dataset (images and labels)\n","train_dataset = torchvision.datasets.MNIST(root='../../data', \n","                                           train=True, \n","                                           transform=transforms.ToTensor(),\n","                                           download=True)\n","print(len(train_loader))\n","\n","test_dataset = torchvision.datasets.MNIST(root='../../data', \n","                                          train=False, \n","                                          transform=transforms.ToTensor())\n","print(len(test_dataset))\n","\n","# Data loader (input pipeline)\n","train_loader = torch.utils.data.DataLoader(dataset=train_dataset, \n","                                           batch_size=batch_size, \n","                                           shuffle=True)\n","\n","test_loader = torch.utils.data.DataLoader(dataset=test_dataset, \n","                                          batch_size=batch_size, \n","                                          shuffle=False)\n","\n","# Logistic regression model\n","model = nn.Linear(input_size, num_classes)\n","\n","# Loss and optimizer\n","# nn.CrossEntropyLoss() computes softmax internally\n","criterion = nn.CrossEntropyLoss()  \n","optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)  \n","\n","# Train the model\n","total_step = len(train_loader)\n","for epoch in range(num_epochs):\n","    for i, (images, labels) in enumerate(train_loader):\n","        # Reshape images to (batch_size, input_size)\n","        images = images.reshape(-1, 28*28)\n","        \n","        # Forward pass\n","        outputs = model(images)\n","        loss = criterion(outputs, labels)\n","        \n","        # Backward and optimize\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step() # 这一句是apply所有的梯度以更新parameter的值，这里不需要传入梯度，因为梯度的引用已经在其构造函数中传入的parameters中包含。\n","        \n","        if (i+1) % 100 == 0:\n","            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n","                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n","\n","# Test the model\n","# In test phase, we don't need to compute gradients (for memory efficiency)\n","print(\"labels size:\", labels.size(0))\n","with torch.no_grad():\n","    correct = 0\n","    total = 0\n","    for images, labels in test_loader:\n","        images = images.reshape(-1, 28*28)\n","        outputs = model(images)\n","        _, predicted = torch.max(outputs.data, 1)\n","        total += labels.size(0)\n","        correct += (predicted == labels).sum()\n","\n","    print('Accuracy of the model on the 10000 test images: {} %'.format(100 * correct / total))\n","\n","# Save the model checkpoint\n","torch.save(model.state_dict(), 'model.ckpt')"],"execution_count":5,"outputs":[{"output_type":"stream","text":["600\n","10000\n","Epoch [1/10], Step [100/600], Loss: 2.1726\n","Epoch [1/10], Step [200/600], Loss: 2.1246\n","Epoch [1/10], Step [300/600], Loss: 2.0230\n","Epoch [1/10], Step [400/600], Loss: 1.9478\n","Epoch [1/10], Step [500/600], Loss: 1.8760\n","Epoch [1/10], Step [600/600], Loss: 1.8004\n","Epoch [2/10], Step [100/600], Loss: 1.7316\n","Epoch [2/10], Step [200/600], Loss: 1.6338\n","Epoch [2/10], Step [300/600], Loss: 1.6288\n","Epoch [2/10], Step [400/600], Loss: 1.5439\n","Epoch [2/10], Step [500/600], Loss: 1.4867\n","Epoch [2/10], Step [600/600], Loss: 1.4799\n","Epoch [3/10], Step [100/600], Loss: 1.3890\n","Epoch [3/10], Step [200/600], Loss: 1.3548\n","Epoch [3/10], Step [300/600], Loss: 1.3412\n","Epoch [3/10], Step [400/600], Loss: 1.2939\n","Epoch [3/10], Step [500/600], Loss: 1.1689\n","Epoch [3/10], Step [600/600], Loss: 1.2375\n","Epoch [4/10], Step [100/600], Loss: 1.3300\n","Epoch [4/10], Step [200/600], Loss: 1.2772\n","Epoch [4/10], Step [300/600], Loss: 1.2073\n","Epoch [4/10], Step [400/600], Loss: 1.1630\n","Epoch [4/10], Step [500/600], Loss: 1.1366\n","Epoch [4/10], Step [600/600], Loss: 1.2448\n","Epoch [5/10], Step [100/600], Loss: 1.0103\n","Epoch [5/10], Step [200/600], Loss: 1.0316\n","Epoch [5/10], Step [300/600], Loss: 1.0427\n","Epoch [5/10], Step [400/600], Loss: 1.0344\n","Epoch [5/10], Step [500/600], Loss: 1.0955\n","Epoch [5/10], Step [600/600], Loss: 1.0358\n","Epoch [6/10], Step [100/600], Loss: 1.0115\n","Epoch [6/10], Step [200/600], Loss: 0.9666\n","Epoch [6/10], Step [300/600], Loss: 1.0815\n","Epoch [6/10], Step [400/600], Loss: 1.0241\n","Epoch [6/10], Step [500/600], Loss: 0.8646\n","Epoch [6/10], Step [600/600], Loss: 0.9368\n","Epoch [7/10], Step [100/600], Loss: 0.9405\n","Epoch [7/10], Step [200/600], Loss: 0.7854\n","Epoch [7/10], Step [300/600], Loss: 0.8262\n","Epoch [7/10], Step [400/600], Loss: 0.7952\n","Epoch [7/10], Step [500/600], Loss: 0.8650\n","Epoch [7/10], Step [600/600], Loss: 0.8649\n","Epoch [8/10], Step [100/600], Loss: 0.9175\n","Epoch [8/10], Step [200/600], Loss: 0.8842\n","Epoch [8/10], Step [300/600], Loss: 0.8977\n","Epoch [8/10], Step [400/600], Loss: 0.8797\n","Epoch [8/10], Step [500/600], Loss: 0.8236\n","Epoch [8/10], Step [600/600], Loss: 0.8981\n","Epoch [9/10], Step [100/600], Loss: 0.8702\n","Epoch [9/10], Step [200/600], Loss: 0.7234\n","Epoch [9/10], Step [300/600], Loss: 0.7894\n","Epoch [9/10], Step [400/600], Loss: 0.8223\n","Epoch [9/10], Step [500/600], Loss: 0.7826\n","Epoch [9/10], Step [600/600], Loss: 0.8211\n","Epoch [10/10], Step [100/600], Loss: 0.7043\n","Epoch [10/10], Step [200/600], Loss: 0.7618\n","Epoch [10/10], Step [300/600], Loss: 0.8554\n","Epoch [10/10], Step [400/600], Loss: 0.6413\n","Epoch [10/10], Step [500/600], Loss: 0.7400\n","Epoch [10/10], Step [600/600], Loss: 0.7462\n","labels size: 100\n","Accuracy of the model on the 10000 test images: 85 %\n"],"name":"stdout"}]}]}