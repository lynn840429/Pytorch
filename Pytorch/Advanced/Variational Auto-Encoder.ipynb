{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Variational Auto-Encoder.ipynb","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"1I9lEXOjMJ1t","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"f0b22816-66f4-4e75-82ae-9d75465ae401","executionInfo":{"status":"ok","timestamp":1575133355111,"user_tz":-480,"elapsed":226135,"user":{"displayName":"Lynn","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAA3iweeMBLyNHjfWr2cL84enkEcjNXptNrvoZJUw=s64","userId":"13191593999274530441"}}},"source":["import os\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torchvision\n","from torchvision import transforms\n","from torchvision.utils import save_image\n","\n","\n","# Device configuration\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","# Create a directory if not exists\n","sample_dir = 'samples'\n","if not os.path.exists(sample_dir):\n","    os.makedirs(sample_dir)\n","\n","# Hyper-parameters\n","image_size = 784\n","h_dim = 400\n","z_dim = 20\n","num_epochs = 15\n","batch_size = 128\n","learning_rate = 1e-3\n","\n","# MNIST dataset\n","dataset = torchvision.datasets.MNIST(root='../../data',\n","                                     train=True,\n","                                     transform=transforms.ToTensor(),\n","                                     download=True)\n","\n","# Data loader\n","data_loader = torch.utils.data.DataLoader(dataset=dataset,\n","                                          batch_size=batch_size, \n","                                          shuffle=True)\n","\n","\n","# VAE model\n","class VAE(nn.Module):\n","    def __init__(self, image_size=784, h_dim=400, z_dim=20):\n","        super(VAE, self).__init__()\n","        self.fc1 = nn.Linear(image_size, h_dim)\n","        self.fc2 = nn.Linear(h_dim, z_dim)\n","        self.fc3 = nn.Linear(h_dim, z_dim)\n","        self.fc4 = nn.Linear(z_dim, h_dim)\n","        self.fc5 = nn.Linear(h_dim, image_size)\n","        \n","    def encode(self, x):\n","        h = F.relu(self.fc1(x))\n","        return self.fc2(h), self.fc3(h)\n","    \n","    def reparameterize(self, mu, log_var):\n","        std = torch.exp(log_var/2)\n","        eps = torch.randn_like(std)\n","        return mu + eps * std\n","\n","    def decode(self, z):\n","        h = F.relu(self.fc4(z))\n","        return F.sigmoid(self.fc5(h))\n","    \n","    def forward(self, x):\n","        mu, log_var = self.encode(x)\n","        z = self.reparameterize(mu, log_var)\n","        x_reconst = self.decode(z)\n","        return x_reconst, mu, log_var\n","\n","model = VAE().to(device)\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","\n","# Start training\n","for epoch in range(num_epochs):\n","    for i, (x, _) in enumerate(data_loader):\n","        # Forward pass\n","        x = x.to(device).view(-1, image_size)\n","        x_reconst, mu, log_var = model(x)\n","        \n","        # Compute reconstruction loss and kl divergence\n","        # For KL divergence, see Appendix B in VAE paper or http://yunjey47.tistory.com/43\n","        reconst_loss = F.binary_cross_entropy(x_reconst, x, size_average=False)\n","        kl_div = - 0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n","        \n","        # Backprop and optimize\n","        loss = reconst_loss + kl_div\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","        \n","        if (i+1) % 10 == 0:\n","            print (\"Epoch[{}/{}], Step [{}/{}], Reconst Loss: {:.4f}, KL Div: {:.4f}\" \n","                   .format(epoch+1, num_epochs, i+1, len(data_loader), reconst_loss.item(), kl_div.item()))\n","    \n","    with torch.no_grad():\n","        # Save the sampled images\n","        z = torch.randn(batch_size, z_dim).to(device)\n","        out = model.decode(z).view(-1, 1, 28, 28)\n","        save_image(out, os.path.join(sample_dir, 'sampled-{}.png'.format(epoch+1)))\n","\n","        # Save the reconstructed images\n","        out, _, _ = model(x)\n","        x_concat = torch.cat([x.view(-1, 1, 28, 28), out.view(-1, 1, 28, 28)], dim=3)\n","        save_image(x_concat, os.path.join(sample_dir, 'reconst-{}.png'.format(epoch+1)))"],"execution_count":1,"outputs":[{"output_type":"stream","text":["  0%|          | 0/9912422 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ../../data/MNIST/raw/train-images-idx3-ubyte.gz\n"],"name":"stdout"},{"output_type":"stream","text":["9920512it [00:00, 21097218.48it/s]                            \n"],"name":"stderr"},{"output_type":"stream","text":["Extracting ../../data/MNIST/raw/train-images-idx3-ubyte.gz to ../../data/MNIST/raw\n"],"name":"stdout"},{"output_type":"stream","text":["32768it [00:00, 326466.85it/s]\n","0it [00:00, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ../../data/MNIST/raw/train-labels-idx1-ubyte.gz\n","Extracting ../../data/MNIST/raw/train-labels-idx1-ubyte.gz to ../../data/MNIST/raw\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ../../data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"],"name":"stdout"},{"output_type":"stream","text":["1654784it [00:00, 5605621.20it/s]                           \n","8192it [00:00, 126917.96it/s]\n"],"name":"stderr"},{"output_type":"stream","text":["Extracting ../../data/MNIST/raw/t10k-images-idx3-ubyte.gz to ../../data/MNIST/raw\n","Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ../../data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n","Extracting ../../data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ../../data/MNIST/raw\n","Processing...\n","Done!\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/torch/nn/functional.py:1351: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n","  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n","/usr/local/lib/python3.6/dist-packages/torch/nn/_reduction.py:43: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n","  warnings.warn(warning.format(ret))\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch[1/15], Step [10/469], Reconst Loss: 34556.0898, KL Div: 3838.6174\n","Epoch[1/15], Step [20/469], Reconst Loss: 30665.5078, KL Div: 1180.0657\n","Epoch[1/15], Step [30/469], Reconst Loss: 27868.5840, KL Div: 1422.6146\n","Epoch[1/15], Step [40/469], Reconst Loss: 26802.0938, KL Div: 682.6030\n","Epoch[1/15], Step [50/469], Reconst Loss: 26511.5605, KL Div: 752.8119\n","Epoch[1/15], Step [60/469], Reconst Loss: 25347.3438, KL Div: 777.0775\n","Epoch[1/15], Step [70/469], Reconst Loss: 24344.0410, KL Div: 1041.1051\n","Epoch[1/15], Step [80/469], Reconst Loss: 22790.5781, KL Div: 1393.7916\n","Epoch[1/15], Step [90/469], Reconst Loss: 21845.6836, KL Div: 1350.9868\n","Epoch[1/15], Step [100/469], Reconst Loss: 21459.4746, KL Div: 1432.8992\n","Epoch[1/15], Step [110/469], Reconst Loss: 20353.9297, KL Div: 1639.5797\n","Epoch[1/15], Step [120/469], Reconst Loss: 19126.9316, KL Div: 1647.4448\n","Epoch[1/15], Step [130/469], Reconst Loss: 18735.1836, KL Div: 1899.7988\n","Epoch[1/15], Step [140/469], Reconst Loss: 18917.8867, KL Div: 1845.6575\n","Epoch[1/15], Step [150/469], Reconst Loss: 19634.8105, KL Div: 1855.4011\n","Epoch[1/15], Step [160/469], Reconst Loss: 18514.7734, KL Div: 1791.8005\n","Epoch[1/15], Step [170/469], Reconst Loss: 18321.1719, KL Div: 1948.5251\n","Epoch[1/15], Step [180/469], Reconst Loss: 17212.6973, KL Div: 1961.2224\n","Epoch[1/15], Step [190/469], Reconst Loss: 17694.7031, KL Div: 2043.4142\n","Epoch[1/15], Step [200/469], Reconst Loss: 17652.0605, KL Div: 1937.9846\n","Epoch[1/15], Step [210/469], Reconst Loss: 17212.5879, KL Div: 2072.1069\n","Epoch[1/15], Step [220/469], Reconst Loss: 16995.6074, KL Div: 2189.5698\n","Epoch[1/15], Step [230/469], Reconst Loss: 16719.3203, KL Div: 1965.5742\n","Epoch[1/15], Step [240/469], Reconst Loss: 17143.9355, KL Div: 2188.3606\n","Epoch[1/15], Step [250/469], Reconst Loss: 16835.3457, KL Div: 2054.3230\n","Epoch[1/15], Step [260/469], Reconst Loss: 14946.4287, KL Div: 2289.0623\n","Epoch[1/15], Step [270/469], Reconst Loss: 16560.3535, KL Div: 2272.0771\n","Epoch[1/15], Step [280/469], Reconst Loss: 16143.3486, KL Div: 2326.7847\n","Epoch[1/15], Step [290/469], Reconst Loss: 14878.4961, KL Div: 2321.1416\n","Epoch[1/15], Step [300/469], Reconst Loss: 15729.5039, KL Div: 2237.0679\n","Epoch[1/15], Step [310/469], Reconst Loss: 15729.0029, KL Div: 2379.0623\n","Epoch[1/15], Step [320/469], Reconst Loss: 14231.1514, KL Div: 2494.4199\n","Epoch[1/15], Step [330/469], Reconst Loss: 15016.6348, KL Div: 2381.6655\n","Epoch[1/15], Step [340/469], Reconst Loss: 14742.9795, KL Div: 2413.2808\n","Epoch[1/15], Step [350/469], Reconst Loss: 15880.3721, KL Div: 2401.9319\n","Epoch[1/15], Step [360/469], Reconst Loss: 15244.4668, KL Div: 2393.4692\n","Epoch[1/15], Step [370/469], Reconst Loss: 14361.0029, KL Div: 2529.8066\n","Epoch[1/15], Step [380/469], Reconst Loss: 15008.5010, KL Div: 2535.6885\n","Epoch[1/15], Step [390/469], Reconst Loss: 13983.8887, KL Div: 2603.1394\n","Epoch[1/15], Step [400/469], Reconst Loss: 13794.0898, KL Div: 2635.4714\n","Epoch[1/15], Step [410/469], Reconst Loss: 14217.4434, KL Div: 2502.6489\n","Epoch[1/15], Step [420/469], Reconst Loss: 14246.1396, KL Div: 2551.8855\n","Epoch[1/15], Step [430/469], Reconst Loss: 13357.1943, KL Div: 2624.0134\n","Epoch[1/15], Step [440/469], Reconst Loss: 14038.9180, KL Div: 2700.6448\n","Epoch[1/15], Step [450/469], Reconst Loss: 13175.9082, KL Div: 2731.4753\n","Epoch[1/15], Step [460/469], Reconst Loss: 13895.1152, KL Div: 2597.8235\n","Epoch[2/15], Step [10/469], Reconst Loss: 14160.2432, KL Div: 2674.3955\n","Epoch[2/15], Step [20/469], Reconst Loss: 14309.7344, KL Div: 2684.9414\n","Epoch[2/15], Step [30/469], Reconst Loss: 13861.8955, KL Div: 2712.0337\n","Epoch[2/15], Step [40/469], Reconst Loss: 14575.2979, KL Div: 2769.6938\n","Epoch[2/15], Step [50/469], Reconst Loss: 13457.1133, KL Div: 2655.9817\n","Epoch[2/15], Step [60/469], Reconst Loss: 13884.2949, KL Div: 2684.6599\n","Epoch[2/15], Step [70/469], Reconst Loss: 13665.4600, KL Div: 2663.5955\n","Epoch[2/15], Step [80/469], Reconst Loss: 13621.2490, KL Div: 2705.7915\n","Epoch[2/15], Step [90/469], Reconst Loss: 12428.2461, KL Div: 2720.6611\n","Epoch[2/15], Step [100/469], Reconst Loss: 13005.6631, KL Div: 2668.0640\n","Epoch[2/15], Step [110/469], Reconst Loss: 13004.1816, KL Div: 2777.7124\n","Epoch[2/15], Step [120/469], Reconst Loss: 13015.0723, KL Div: 2787.8765\n","Epoch[2/15], Step [130/469], Reconst Loss: 13177.8350, KL Div: 2837.1970\n","Epoch[2/15], Step [140/469], Reconst Loss: 12751.9072, KL Div: 2723.4392\n","Epoch[2/15], Step [150/469], Reconst Loss: 12482.2939, KL Div: 2923.5601\n","Epoch[2/15], Step [160/469], Reconst Loss: 12713.1475, KL Div: 2838.5588\n","Epoch[2/15], Step [170/469], Reconst Loss: 13206.7939, KL Div: 2839.4836\n","Epoch[2/15], Step [180/469], Reconst Loss: 12340.4258, KL Div: 2980.7568\n","Epoch[2/15], Step [190/469], Reconst Loss: 11848.2832, KL Div: 2705.5063\n","Epoch[2/15], Step [200/469], Reconst Loss: 13096.3096, KL Div: 2854.9067\n","Epoch[2/15], Step [210/469], Reconst Loss: 12308.2109, KL Div: 2832.3828\n","Epoch[2/15], Step [220/469], Reconst Loss: 12517.0000, KL Div: 2878.3755\n","Epoch[2/15], Step [230/469], Reconst Loss: 12840.7500, KL Div: 2810.5422\n","Epoch[2/15], Step [240/469], Reconst Loss: 12319.2861, KL Div: 2931.6167\n","Epoch[2/15], Step [250/469], Reconst Loss: 12248.8105, KL Div: 2791.4126\n","Epoch[2/15], Step [260/469], Reconst Loss: 12539.5811, KL Div: 3110.3711\n","Epoch[2/15], Step [270/469], Reconst Loss: 12664.3193, KL Div: 2789.6265\n","Epoch[2/15], Step [280/469], Reconst Loss: 12879.4893, KL Div: 2896.6707\n","Epoch[2/15], Step [290/469], Reconst Loss: 12470.1094, KL Div: 2970.6616\n","Epoch[2/15], Step [300/469], Reconst Loss: 12215.6191, KL Div: 2897.7908\n","Epoch[2/15], Step [310/469], Reconst Loss: 12842.3135, KL Div: 2915.7791\n","Epoch[2/15], Step [320/469], Reconst Loss: 12604.9883, KL Div: 2930.6060\n","Epoch[2/15], Step [330/469], Reconst Loss: 12750.2998, KL Div: 2959.9211\n","Epoch[2/15], Step [340/469], Reconst Loss: 13103.6377, KL Div: 2943.7673\n","Epoch[2/15], Step [350/469], Reconst Loss: 11994.8594, KL Div: 2878.8154\n","Epoch[2/15], Step [360/469], Reconst Loss: 11793.8428, KL Div: 2973.9109\n","Epoch[2/15], Step [370/469], Reconst Loss: 12178.4668, KL Div: 2856.3572\n","Epoch[2/15], Step [380/469], Reconst Loss: 12066.3945, KL Div: 3132.1980\n","Epoch[2/15], Step [390/469], Reconst Loss: 11920.6240, KL Div: 2983.5723\n","Epoch[2/15], Step [400/469], Reconst Loss: 12887.7412, KL Div: 2979.1597\n","Epoch[2/15], Step [410/469], Reconst Loss: 12260.4287, KL Div: 2932.7031\n","Epoch[2/15], Step [420/469], Reconst Loss: 11648.1787, KL Div: 2869.6377\n","Epoch[2/15], Step [430/469], Reconst Loss: 11709.9805, KL Div: 3022.7637\n","Epoch[2/15], Step [440/469], Reconst Loss: 12113.8955, KL Div: 2918.9353\n","Epoch[2/15], Step [450/469], Reconst Loss: 12417.7324, KL Div: 2878.4104\n","Epoch[2/15], Step [460/469], Reconst Loss: 12036.2178, KL Div: 2951.5066\n","Epoch[3/15], Step [10/469], Reconst Loss: 11925.7852, KL Div: 2929.2556\n","Epoch[3/15], Step [20/469], Reconst Loss: 12448.1270, KL Div: 3020.4253\n","Epoch[3/15], Step [30/469], Reconst Loss: 11496.6025, KL Div: 2965.3684\n","Epoch[3/15], Step [40/469], Reconst Loss: 12170.0928, KL Div: 3068.8823\n","Epoch[3/15], Step [50/469], Reconst Loss: 12401.4824, KL Div: 2900.4797\n","Epoch[3/15], Step [60/469], Reconst Loss: 11211.9199, KL Div: 2994.0239\n","Epoch[3/15], Step [70/469], Reconst Loss: 12350.4092, KL Div: 3092.5183\n","Epoch[3/15], Step [80/469], Reconst Loss: 12135.0703, KL Div: 2956.8228\n","Epoch[3/15], Step [90/469], Reconst Loss: 11840.9648, KL Div: 2975.0205\n","Epoch[3/15], Step [100/469], Reconst Loss: 11822.7168, KL Div: 3134.8044\n","Epoch[3/15], Step [110/469], Reconst Loss: 11381.2705, KL Div: 3054.2986\n","Epoch[3/15], Step [120/469], Reconst Loss: 11733.3447, KL Div: 3023.6992\n","Epoch[3/15], Step [130/469], Reconst Loss: 11975.3662, KL Div: 2999.0007\n","Epoch[3/15], Step [140/469], Reconst Loss: 12184.4473, KL Div: 3039.2729\n","Epoch[3/15], Step [150/469], Reconst Loss: 11751.6406, KL Div: 3121.1543\n","Epoch[3/15], Step [160/469], Reconst Loss: 11474.5830, KL Div: 2951.9478\n","Epoch[3/15], Step [170/469], Reconst Loss: 10953.9775, KL Div: 3132.1006\n","Epoch[3/15], Step [180/469], Reconst Loss: 11367.6865, KL Div: 2986.0005\n","Epoch[3/15], Step [190/469], Reconst Loss: 11550.1191, KL Div: 3161.9797\n","Epoch[3/15], Step [200/469], Reconst Loss: 11960.3672, KL Div: 3061.2686\n","Epoch[3/15], Step [210/469], Reconst Loss: 11551.2129, KL Div: 3013.5642\n","Epoch[3/15], Step [220/469], Reconst Loss: 11521.4043, KL Div: 3155.4077\n","Epoch[3/15], Step [230/469], Reconst Loss: 11644.5576, KL Div: 3025.5874\n","Epoch[3/15], Step [240/469], Reconst Loss: 12171.4746, KL Div: 3160.6274\n","Epoch[3/15], Step [250/469], Reconst Loss: 12347.7119, KL Div: 3120.3167\n","Epoch[3/15], Step [260/469], Reconst Loss: 11894.0361, KL Div: 3199.9827\n","Epoch[3/15], Step [270/469], Reconst Loss: 11622.3877, KL Div: 3105.9497\n","Epoch[3/15], Step [280/469], Reconst Loss: 11064.0674, KL Div: 3023.0620\n","Epoch[3/15], Step [290/469], Reconst Loss: 11813.1035, KL Div: 3081.2290\n","Epoch[3/15], Step [300/469], Reconst Loss: 11578.7012, KL Div: 2982.2148\n","Epoch[3/15], Step [310/469], Reconst Loss: 11555.4014, KL Div: 3036.1028\n","Epoch[3/15], Step [320/469], Reconst Loss: 11614.3867, KL Div: 3084.0137\n","Epoch[3/15], Step [330/469], Reconst Loss: 11278.8535, KL Div: 3058.0027\n","Epoch[3/15], Step [340/469], Reconst Loss: 11536.3359, KL Div: 3066.2119\n","Epoch[3/15], Step [350/469], Reconst Loss: 11600.7529, KL Div: 3106.7627\n","Epoch[3/15], Step [360/469], Reconst Loss: 11028.9746, KL Div: 3024.4695\n","Epoch[3/15], Step [370/469], Reconst Loss: 11206.6768, KL Div: 3085.4753\n","Epoch[3/15], Step [380/469], Reconst Loss: 11446.5020, KL Div: 3100.2554\n","Epoch[3/15], Step [390/469], Reconst Loss: 11804.3164, KL Div: 3095.7510\n","Epoch[3/15], Step [400/469], Reconst Loss: 11793.9912, KL Div: 3214.4277\n","Epoch[3/15], Step [410/469], Reconst Loss: 11764.6602, KL Div: 3115.8940\n","Epoch[3/15], Step [420/469], Reconst Loss: 11539.7432, KL Div: 3106.5100\n","Epoch[3/15], Step [430/469], Reconst Loss: 11566.3096, KL Div: 3169.1960\n","Epoch[3/15], Step [440/469], Reconst Loss: 11234.5449, KL Div: 3060.6223\n","Epoch[3/15], Step [450/469], Reconst Loss: 11438.9121, KL Div: 3155.7610\n","Epoch[3/15], Step [460/469], Reconst Loss: 11521.8232, KL Div: 3154.3174\n","Epoch[4/15], Step [10/469], Reconst Loss: 11180.1006, KL Div: 3083.1636\n","Epoch[4/15], Step [20/469], Reconst Loss: 10882.2568, KL Div: 3015.8770\n","Epoch[4/15], Step [30/469], Reconst Loss: 11600.4561, KL Div: 3194.7869\n","Epoch[4/15], Step [40/469], Reconst Loss: 11592.4375, KL Div: 3189.7168\n","Epoch[4/15], Step [50/469], Reconst Loss: 11483.8818, KL Div: 3202.6638\n","Epoch[4/15], Step [60/469], Reconst Loss: 10997.3447, KL Div: 3109.8171\n","Epoch[4/15], Step [70/469], Reconst Loss: 11567.7168, KL Div: 2979.8833\n","Epoch[4/15], Step [80/469], Reconst Loss: 11442.6074, KL Div: 3272.9641\n","Epoch[4/15], Step [90/469], Reconst Loss: 11863.9717, KL Div: 3016.1731\n","Epoch[4/15], Step [100/469], Reconst Loss: 11129.7236, KL Div: 3189.2034\n","Epoch[4/15], Step [110/469], Reconst Loss: 10975.2393, KL Div: 3026.1770\n","Epoch[4/15], Step [120/469], Reconst Loss: 11199.0986, KL Div: 3149.3130\n","Epoch[4/15], Step [130/469], Reconst Loss: 10647.0166, KL Div: 3032.1445\n","Epoch[4/15], Step [140/469], Reconst Loss: 11478.2334, KL Div: 3015.9602\n","Epoch[4/15], Step [150/469], Reconst Loss: 12022.1035, KL Div: 3290.2578\n","Epoch[4/15], Step [160/469], Reconst Loss: 11182.0176, KL Div: 3093.5073\n","Epoch[4/15], Step [170/469], Reconst Loss: 11211.1416, KL Div: 3090.7300\n","Epoch[4/15], Step [180/469], Reconst Loss: 11311.4971, KL Div: 3241.2305\n","Epoch[4/15], Step [190/469], Reconst Loss: 11370.7070, KL Div: 3206.8987\n","Epoch[4/15], Step [200/469], Reconst Loss: 11462.1680, KL Div: 3067.1675\n","Epoch[4/15], Step [210/469], Reconst Loss: 11185.2695, KL Div: 3082.5488\n","Epoch[4/15], Step [220/469], Reconst Loss: 11786.4365, KL Div: 3140.2800\n","Epoch[4/15], Step [230/469], Reconst Loss: 11242.2168, KL Div: 3189.1089\n","Epoch[4/15], Step [240/469], Reconst Loss: 11068.5186, KL Div: 3191.1040\n","Epoch[4/15], Step [250/469], Reconst Loss: 10572.8564, KL Div: 3075.4622\n","Epoch[4/15], Step [260/469], Reconst Loss: 11183.8125, KL Div: 3175.7178\n","Epoch[4/15], Step [270/469], Reconst Loss: 11383.7803, KL Div: 3296.8018\n","Epoch[4/15], Step [280/469], Reconst Loss: 11487.2441, KL Div: 3222.2842\n","Epoch[4/15], Step [290/469], Reconst Loss: 11168.2148, KL Div: 3197.9363\n","Epoch[4/15], Step [300/469], Reconst Loss: 10974.2705, KL Div: 3099.2712\n","Epoch[4/15], Step [310/469], Reconst Loss: 10254.9648, KL Div: 3062.3140\n","Epoch[4/15], Step [320/469], Reconst Loss: 11045.2041, KL Div: 3120.6216\n","Epoch[4/15], Step [330/469], Reconst Loss: 10658.4639, KL Div: 3068.7903\n","Epoch[4/15], Step [340/469], Reconst Loss: 11279.3291, KL Div: 3144.4873\n","Epoch[4/15], Step [350/469], Reconst Loss: 10686.3965, KL Div: 3158.8984\n","Epoch[4/15], Step [360/469], Reconst Loss: 11390.0684, KL Div: 3111.3335\n","Epoch[4/15], Step [370/469], Reconst Loss: 10684.6104, KL Div: 3163.6182\n","Epoch[4/15], Step [380/469], Reconst Loss: 10640.4180, KL Div: 3108.7578\n","Epoch[4/15], Step [390/469], Reconst Loss: 11233.3184, KL Div: 3179.6924\n","Epoch[4/15], Step [400/469], Reconst Loss: 11027.8965, KL Div: 3301.1191\n","Epoch[4/15], Step [410/469], Reconst Loss: 10313.4404, KL Div: 3057.5244\n","Epoch[4/15], Step [420/469], Reconst Loss: 11272.1816, KL Div: 3164.2285\n","Epoch[4/15], Step [430/469], Reconst Loss: 11225.2686, KL Div: 3199.3191\n","Epoch[4/15], Step [440/469], Reconst Loss: 11177.9971, KL Div: 3206.1875\n","Epoch[4/15], Step [450/469], Reconst Loss: 10814.0312, KL Div: 3122.5605\n","Epoch[4/15], Step [460/469], Reconst Loss: 10918.8301, KL Div: 3147.8440\n","Epoch[5/15], Step [10/469], Reconst Loss: 11087.4648, KL Div: 3350.3411\n","Epoch[5/15], Step [20/469], Reconst Loss: 11355.3359, KL Div: 3129.6267\n","Epoch[5/15], Step [30/469], Reconst Loss: 10926.9980, KL Div: 3173.6050\n","Epoch[5/15], Step [40/469], Reconst Loss: 10614.5361, KL Div: 3222.8140\n","Epoch[5/15], Step [50/469], Reconst Loss: 11193.5654, KL Div: 3146.2400\n","Epoch[5/15], Step [60/469], Reconst Loss: 10807.1123, KL Div: 3098.7412\n","Epoch[5/15], Step [70/469], Reconst Loss: 10979.6006, KL Div: 3182.4863\n","Epoch[5/15], Step [80/469], Reconst Loss: 11074.2207, KL Div: 3129.7729\n","Epoch[5/15], Step [90/469], Reconst Loss: 11021.4502, KL Div: 3202.7207\n","Epoch[5/15], Step [100/469], Reconst Loss: 10814.3730, KL Div: 3265.4890\n","Epoch[5/15], Step [110/469], Reconst Loss: 11382.4111, KL Div: 3143.5161\n","Epoch[5/15], Step [120/469], Reconst Loss: 11241.7217, KL Div: 3184.1880\n","Epoch[5/15], Step [130/469], Reconst Loss: 10711.5381, KL Div: 3203.9326\n","Epoch[5/15], Step [140/469], Reconst Loss: 11112.1699, KL Div: 3193.1946\n","Epoch[5/15], Step [150/469], Reconst Loss: 10444.6123, KL Div: 3196.5684\n","Epoch[5/15], Step [160/469], Reconst Loss: 10701.7012, KL Div: 3050.4023\n","Epoch[5/15], Step [170/469], Reconst Loss: 11106.8301, KL Div: 3159.0518\n","Epoch[5/15], Step [180/469], Reconst Loss: 10651.1299, KL Div: 3183.8682\n","Epoch[5/15], Step [190/469], Reconst Loss: 10737.7822, KL Div: 3098.5793\n","Epoch[5/15], Step [200/469], Reconst Loss: 10977.0586, KL Div: 3244.4758\n","Epoch[5/15], Step [210/469], Reconst Loss: 11652.0781, KL Div: 3124.4502\n","Epoch[5/15], Step [220/469], Reconst Loss: 11037.1660, KL Div: 3298.5784\n","Epoch[5/15], Step [230/469], Reconst Loss: 10558.4795, KL Div: 3095.0222\n","Epoch[5/15], Step [240/469], Reconst Loss: 10668.9668, KL Div: 3093.5833\n","Epoch[5/15], Step [250/469], Reconst Loss: 11040.1680, KL Div: 3294.7451\n","Epoch[5/15], Step [260/469], Reconst Loss: 11158.7119, KL Div: 3216.3967\n","Epoch[5/15], Step [270/469], Reconst Loss: 11111.2207, KL Div: 3161.6484\n","Epoch[5/15], Step [280/469], Reconst Loss: 10715.7803, KL Div: 3162.6128\n","Epoch[5/15], Step [290/469], Reconst Loss: 11024.9492, KL Div: 3176.2522\n","Epoch[5/15], Step [300/469], Reconst Loss: 10885.7471, KL Div: 3202.5098\n","Epoch[5/15], Step [310/469], Reconst Loss: 11098.3262, KL Div: 3145.9590\n","Epoch[5/15], Step [320/469], Reconst Loss: 10883.2178, KL Div: 3263.5322\n","Epoch[5/15], Step [330/469], Reconst Loss: 10903.6641, KL Div: 3205.2046\n","Epoch[5/15], Step [340/469], Reconst Loss: 10646.8096, KL Div: 3222.4504\n","Epoch[5/15], Step [350/469], Reconst Loss: 10896.7793, KL Div: 3200.0415\n","Epoch[5/15], Step [360/469], Reconst Loss: 11189.5205, KL Div: 3233.6255\n","Epoch[5/15], Step [370/469], Reconst Loss: 10652.7041, KL Div: 3189.3667\n","Epoch[5/15], Step [380/469], Reconst Loss: 11155.4082, KL Div: 3239.6133\n","Epoch[5/15], Step [390/469], Reconst Loss: 10422.9727, KL Div: 3146.3845\n","Epoch[5/15], Step [400/469], Reconst Loss: 11007.2471, KL Div: 3166.7898\n","Epoch[5/15], Step [410/469], Reconst Loss: 10593.6953, KL Div: 3186.9521\n","Epoch[5/15], Step [420/469], Reconst Loss: 11268.4004, KL Div: 3274.3110\n","Epoch[5/15], Step [430/469], Reconst Loss: 11138.2715, KL Div: 3202.1362\n","Epoch[5/15], Step [440/469], Reconst Loss: 10589.3467, KL Div: 3053.3044\n","Epoch[5/15], Step [450/469], Reconst Loss: 10691.1689, KL Div: 3239.4578\n","Epoch[5/15], Step [460/469], Reconst Loss: 10627.4111, KL Div: 3162.0400\n","Epoch[6/15], Step [10/469], Reconst Loss: 11065.2158, KL Div: 3263.9121\n","Epoch[6/15], Step [20/469], Reconst Loss: 10919.0508, KL Div: 3249.0105\n","Epoch[6/15], Step [30/469], Reconst Loss: 10939.2070, KL Div: 3137.1960\n","Epoch[6/15], Step [40/469], Reconst Loss: 10946.6299, KL Div: 3144.5654\n","Epoch[6/15], Step [50/469], Reconst Loss: 10839.2080, KL Div: 3315.8435\n","Epoch[6/15], Step [60/469], Reconst Loss: 10906.5039, KL Div: 3218.7959\n","Epoch[6/15], Step [70/469], Reconst Loss: 10422.9756, KL Div: 3164.4478\n","Epoch[6/15], Step [80/469], Reconst Loss: 10473.5723, KL Div: 3187.1616\n","Epoch[6/15], Step [90/469], Reconst Loss: 11262.6738, KL Div: 3210.6543\n","Epoch[6/15], Step [100/469], Reconst Loss: 10604.4561, KL Div: 3136.0002\n","Epoch[6/15], Step [110/469], Reconst Loss: 10512.3965, KL Div: 3234.5996\n","Epoch[6/15], Step [120/469], Reconst Loss: 10971.0547, KL Div: 3292.3271\n","Epoch[6/15], Step [130/469], Reconst Loss: 10557.9717, KL Div: 3144.4409\n","Epoch[6/15], Step [140/469], Reconst Loss: 10394.7734, KL Div: 3112.7302\n","Epoch[6/15], Step [150/469], Reconst Loss: 10813.4092, KL Div: 3222.1650\n","Epoch[6/15], Step [160/469], Reconst Loss: 11006.5645, KL Div: 3216.8618\n","Epoch[6/15], Step [170/469], Reconst Loss: 10686.3252, KL Div: 3139.1687\n","Epoch[6/15], Step [180/469], Reconst Loss: 10997.1963, KL Div: 3182.2827\n","Epoch[6/15], Step [190/469], Reconst Loss: 11079.6279, KL Div: 3236.2747\n","Epoch[6/15], Step [200/469], Reconst Loss: 10991.1201, KL Div: 3261.6958\n","Epoch[6/15], Step [210/469], Reconst Loss: 10855.7871, KL Div: 3169.2168\n","Epoch[6/15], Step [220/469], Reconst Loss: 10556.2549, KL Div: 3075.9304\n","Epoch[6/15], Step [230/469], Reconst Loss: 10408.7920, KL Div: 3261.3914\n","Epoch[6/15], Step [240/469], Reconst Loss: 10647.1885, KL Div: 3190.6499\n","Epoch[6/15], Step [250/469], Reconst Loss: 10607.1445, KL Div: 3324.0571\n","Epoch[6/15], Step [260/469], Reconst Loss: 11353.7939, KL Div: 3206.7590\n","Epoch[6/15], Step [270/469], Reconst Loss: 10923.2041, KL Div: 3276.0974\n","Epoch[6/15], Step [280/469], Reconst Loss: 10837.5889, KL Div: 3178.5854\n","Epoch[6/15], Step [290/469], Reconst Loss: 10592.7451, KL Div: 3188.8872\n","Epoch[6/15], Step [300/469], Reconst Loss: 10787.9482, KL Div: 3248.8545\n","Epoch[6/15], Step [310/469], Reconst Loss: 10427.5547, KL Div: 3119.1472\n","Epoch[6/15], Step [320/469], Reconst Loss: 11034.8975, KL Div: 3159.1995\n","Epoch[6/15], Step [330/469], Reconst Loss: 10431.6787, KL Div: 3150.5332\n","Epoch[6/15], Step [340/469], Reconst Loss: 10482.4385, KL Div: 3191.9771\n","Epoch[6/15], Step [350/469], Reconst Loss: 10885.4551, KL Div: 3226.9116\n","Epoch[6/15], Step [360/469], Reconst Loss: 10765.7217, KL Div: 3239.4905\n","Epoch[6/15], Step [370/469], Reconst Loss: 10853.2979, KL Div: 3244.1743\n","Epoch[6/15], Step [380/469], Reconst Loss: 10671.6299, KL Div: 3079.2742\n","Epoch[6/15], Step [390/469], Reconst Loss: 10567.0010, KL Div: 3300.7039\n","Epoch[6/15], Step [400/469], Reconst Loss: 10425.8936, KL Div: 3105.0244\n","Epoch[6/15], Step [410/469], Reconst Loss: 10933.3496, KL Div: 3258.1018\n","Epoch[6/15], Step [420/469], Reconst Loss: 11062.7324, KL Div: 3257.1484\n","Epoch[6/15], Step [430/469], Reconst Loss: 10765.1846, KL Div: 3181.4304\n","Epoch[6/15], Step [440/469], Reconst Loss: 10679.6113, KL Div: 3299.1060\n","Epoch[6/15], Step [450/469], Reconst Loss: 10567.2656, KL Div: 3191.2354\n","Epoch[6/15], Step [460/469], Reconst Loss: 10404.1680, KL Div: 3092.1555\n","Epoch[7/15], Step [10/469], Reconst Loss: 11120.7793, KL Div: 3318.3311\n","Epoch[7/15], Step [20/469], Reconst Loss: 10747.1084, KL Div: 3247.9846\n","Epoch[7/15], Step [30/469], Reconst Loss: 10654.9014, KL Div: 3142.3906\n","Epoch[7/15], Step [40/469], Reconst Loss: 10315.4834, KL Div: 3169.7688\n","Epoch[7/15], Step [50/469], Reconst Loss: 10885.9404, KL Div: 3240.2507\n","Epoch[7/15], Step [60/469], Reconst Loss: 11030.3584, KL Div: 3182.5859\n","Epoch[7/15], Step [70/469], Reconst Loss: 10399.1504, KL Div: 3096.0737\n","Epoch[7/15], Step [80/469], Reconst Loss: 10073.4121, KL Div: 3132.8647\n","Epoch[7/15], Step [90/469], Reconst Loss: 10626.2725, KL Div: 3209.9548\n","Epoch[7/15], Step [100/469], Reconst Loss: 10089.3652, KL Div: 3133.5969\n","Epoch[7/15], Step [110/469], Reconst Loss: 10359.4844, KL Div: 3153.0264\n","Epoch[7/15], Step [120/469], Reconst Loss: 10688.0254, KL Div: 3344.5830\n","Epoch[7/15], Step [130/469], Reconst Loss: 11009.9326, KL Div: 3153.6653\n","Epoch[7/15], Step [140/469], Reconst Loss: 10903.8760, KL Div: 3312.8125\n","Epoch[7/15], Step [150/469], Reconst Loss: 10468.2197, KL Div: 3192.4888\n","Epoch[7/15], Step [160/469], Reconst Loss: 10232.8193, KL Div: 3208.9768\n","Epoch[7/15], Step [170/469], Reconst Loss: 10767.7871, KL Div: 3331.3098\n","Epoch[7/15], Step [180/469], Reconst Loss: 10257.7617, KL Div: 3149.9990\n","Epoch[7/15], Step [190/469], Reconst Loss: 10847.0908, KL Div: 3233.5654\n","Epoch[7/15], Step [200/469], Reconst Loss: 10598.8877, KL Div: 3150.5498\n","Epoch[7/15], Step [210/469], Reconst Loss: 11230.9951, KL Div: 3194.0032\n","Epoch[7/15], Step [220/469], Reconst Loss: 10290.9141, KL Div: 3203.8491\n","Epoch[7/15], Step [230/469], Reconst Loss: 10476.5381, KL Div: 3195.3911\n","Epoch[7/15], Step [240/469], Reconst Loss: 10669.5576, KL Div: 3111.2363\n","Epoch[7/15], Step [250/469], Reconst Loss: 10819.0273, KL Div: 3199.9675\n","Epoch[7/15], Step [260/469], Reconst Loss: 10044.9482, KL Div: 3124.0059\n","Epoch[7/15], Step [270/469], Reconst Loss: 10160.3789, KL Div: 3336.2520\n","Epoch[7/15], Step [280/469], Reconst Loss: 9933.6211, KL Div: 3073.2166\n","Epoch[7/15], Step [290/469], Reconst Loss: 10751.1123, KL Div: 3095.1707\n","Epoch[7/15], Step [300/469], Reconst Loss: 10573.8105, KL Div: 3245.6746\n","Epoch[7/15], Step [310/469], Reconst Loss: 10497.9824, KL Div: 3260.2378\n","Epoch[7/15], Step [320/469], Reconst Loss: 9912.7480, KL Div: 3129.5010\n","Epoch[7/15], Step [330/469], Reconst Loss: 10452.9121, KL Div: 3142.3408\n","Epoch[7/15], Step [340/469], Reconst Loss: 10531.5781, KL Div: 3293.5984\n","Epoch[7/15], Step [350/469], Reconst Loss: 10852.5400, KL Div: 3263.3547\n","Epoch[7/15], Step [360/469], Reconst Loss: 10272.9502, KL Div: 3211.5149\n","Epoch[7/15], Step [370/469], Reconst Loss: 10956.4932, KL Div: 3261.6494\n","Epoch[7/15], Step [380/469], Reconst Loss: 10764.9453, KL Div: 3230.9819\n","Epoch[7/15], Step [390/469], Reconst Loss: 10294.2812, KL Div: 3215.8552\n","Epoch[7/15], Step [400/469], Reconst Loss: 10471.8789, KL Div: 3207.8630\n","Epoch[7/15], Step [410/469], Reconst Loss: 11014.4170, KL Div: 3266.2893\n","Epoch[7/15], Step [420/469], Reconst Loss: 10177.4238, KL Div: 3162.7898\n","Epoch[7/15], Step [430/469], Reconst Loss: 9746.2568, KL Div: 3213.1216\n","Epoch[7/15], Step [440/469], Reconst Loss: 9879.4785, KL Div: 3203.4690\n","Epoch[7/15], Step [450/469], Reconst Loss: 10787.5869, KL Div: 3194.6587\n","Epoch[7/15], Step [460/469], Reconst Loss: 10592.9297, KL Div: 3276.7935\n","Epoch[8/15], Step [10/469], Reconst Loss: 10713.7812, KL Div: 3178.8145\n","Epoch[8/15], Step [20/469], Reconst Loss: 10591.8340, KL Div: 3316.2595\n","Epoch[8/15], Step [30/469], Reconst Loss: 10617.5928, KL Div: 3159.0415\n","Epoch[8/15], Step [40/469], Reconst Loss: 10832.9639, KL Div: 3280.6392\n","Epoch[8/15], Step [50/469], Reconst Loss: 10506.7568, KL Div: 3290.1279\n","Epoch[8/15], Step [60/469], Reconst Loss: 11048.5811, KL Div: 3276.8311\n","Epoch[8/15], Step [70/469], Reconst Loss: 10812.5459, KL Div: 3286.5769\n","Epoch[8/15], Step [80/469], Reconst Loss: 10583.9619, KL Div: 3347.7051\n","Epoch[8/15], Step [90/469], Reconst Loss: 10387.2979, KL Div: 3250.4556\n","Epoch[8/15], Step [100/469], Reconst Loss: 10355.9912, KL Div: 3241.7095\n","Epoch[8/15], Step [110/469], Reconst Loss: 10319.9844, KL Div: 3173.6062\n","Epoch[8/15], Step [120/469], Reconst Loss: 10196.1631, KL Div: 3250.0781\n","Epoch[8/15], Step [130/469], Reconst Loss: 10393.9092, KL Div: 3179.1072\n","Epoch[8/15], Step [140/469], Reconst Loss: 10781.8057, KL Div: 3264.2441\n","Epoch[8/15], Step [150/469], Reconst Loss: 10642.0879, KL Div: 3349.0195\n","Epoch[8/15], Step [160/469], Reconst Loss: 10467.2100, KL Div: 3177.6655\n","Epoch[8/15], Step [170/469], Reconst Loss: 10418.5010, KL Div: 3145.9038\n","Epoch[8/15], Step [180/469], Reconst Loss: 10399.1807, KL Div: 3197.9556\n","Epoch[8/15], Step [190/469], Reconst Loss: 10532.4131, KL Div: 3284.4629\n","Epoch[8/15], Step [200/469], Reconst Loss: 10551.7461, KL Div: 3298.3506\n","Epoch[8/15], Step [210/469], Reconst Loss: 10565.6875, KL Div: 3144.3076\n","Epoch[8/15], Step [220/469], Reconst Loss: 10599.0195, KL Div: 3186.0168\n","Epoch[8/15], Step [230/469], Reconst Loss: 10561.3018, KL Div: 3294.7458\n","Epoch[8/15], Step [240/469], Reconst Loss: 10936.3223, KL Div: 3277.5750\n","Epoch[8/15], Step [250/469], Reconst Loss: 10261.1553, KL Div: 3176.2700\n","Epoch[8/15], Step [260/469], Reconst Loss: 10211.4922, KL Div: 3214.1272\n","Epoch[8/15], Step [270/469], Reconst Loss: 10289.7852, KL Div: 3258.4861\n","Epoch[8/15], Step [280/469], Reconst Loss: 10597.4238, KL Div: 3255.1416\n","Epoch[8/15], Step [290/469], Reconst Loss: 10976.3018, KL Div: 3335.5728\n","Epoch[8/15], Step [300/469], Reconst Loss: 10630.0078, KL Div: 3141.9949\n","Epoch[8/15], Step [310/469], Reconst Loss: 10587.2461, KL Div: 3271.4736\n","Epoch[8/15], Step [320/469], Reconst Loss: 10518.6709, KL Div: 3181.5425\n","Epoch[8/15], Step [330/469], Reconst Loss: 10661.5654, KL Div: 3216.2300\n","Epoch[8/15], Step [340/469], Reconst Loss: 10788.2061, KL Div: 3289.2295\n","Epoch[8/15], Step [350/469], Reconst Loss: 10704.4922, KL Div: 3236.5867\n","Epoch[8/15], Step [360/469], Reconst Loss: 10408.7715, KL Div: 3216.3364\n","Epoch[8/15], Step [370/469], Reconst Loss: 10973.9961, KL Div: 3356.1570\n","Epoch[8/15], Step [380/469], Reconst Loss: 10473.7705, KL Div: 3230.1931\n","Epoch[8/15], Step [390/469], Reconst Loss: 10484.5664, KL Div: 3222.1755\n","Epoch[8/15], Step [400/469], Reconst Loss: 10474.4795, KL Div: 3210.0552\n","Epoch[8/15], Step [410/469], Reconst Loss: 10555.5742, KL Div: 3127.3213\n","Epoch[8/15], Step [420/469], Reconst Loss: 10384.5898, KL Div: 3249.6904\n","Epoch[8/15], Step [430/469], Reconst Loss: 10315.8438, KL Div: 3167.1846\n","Epoch[8/15], Step [440/469], Reconst Loss: 9886.9219, KL Div: 3258.2053\n","Epoch[8/15], Step [450/469], Reconst Loss: 10937.2070, KL Div: 3291.3728\n","Epoch[8/15], Step [460/469], Reconst Loss: 10300.6689, KL Div: 3233.1777\n","Epoch[9/15], Step [10/469], Reconst Loss: 10468.9336, KL Div: 3284.9575\n","Epoch[9/15], Step [20/469], Reconst Loss: 10380.9141, KL Div: 3165.4741\n","Epoch[9/15], Step [30/469], Reconst Loss: 10007.3477, KL Div: 3217.7715\n","Epoch[9/15], Step [40/469], Reconst Loss: 10470.3174, KL Div: 3324.7554\n","Epoch[9/15], Step [50/469], Reconst Loss: 10537.6826, KL Div: 3267.9214\n","Epoch[9/15], Step [60/469], Reconst Loss: 10339.1553, KL Div: 3242.1118\n","Epoch[9/15], Step [70/469], Reconst Loss: 9972.8633, KL Div: 3159.3235\n","Epoch[9/15], Step [80/469], Reconst Loss: 10220.6602, KL Div: 3185.7068\n","Epoch[9/15], Step [90/469], Reconst Loss: 10699.1895, KL Div: 3290.1187\n","Epoch[9/15], Step [100/469], Reconst Loss: 9996.5020, KL Div: 3096.0000\n","Epoch[9/15], Step [110/469], Reconst Loss: 10615.3486, KL Div: 3315.4089\n","Epoch[9/15], Step [120/469], Reconst Loss: 10524.6113, KL Div: 3240.2468\n","Epoch[9/15], Step [130/469], Reconst Loss: 10216.7471, KL Div: 3172.6382\n","Epoch[9/15], Step [140/469], Reconst Loss: 10729.0127, KL Div: 3228.9399\n","Epoch[9/15], Step [150/469], Reconst Loss: 10397.9590, KL Div: 3220.2891\n","Epoch[9/15], Step [160/469], Reconst Loss: 10611.7871, KL Div: 3270.0444\n","Epoch[9/15], Step [170/469], Reconst Loss: 10435.1387, KL Div: 3169.4131\n","Epoch[9/15], Step [180/469], Reconst Loss: 10898.7646, KL Div: 3208.2759\n","Epoch[9/15], Step [190/469], Reconst Loss: 10449.0518, KL Div: 3292.7366\n","Epoch[9/15], Step [200/469], Reconst Loss: 10722.9258, KL Div: 3204.9688\n","Epoch[9/15], Step [210/469], Reconst Loss: 10699.3428, KL Div: 3173.8440\n","Epoch[9/15], Step [220/469], Reconst Loss: 10479.2822, KL Div: 3315.6843\n","Epoch[9/15], Step [230/469], Reconst Loss: 10629.1826, KL Div: 3169.7834\n","Epoch[9/15], Step [240/469], Reconst Loss: 10131.3154, KL Div: 3270.5679\n","Epoch[9/15], Step [250/469], Reconst Loss: 10523.2871, KL Div: 3295.3760\n","Epoch[9/15], Step [260/469], Reconst Loss: 11129.9971, KL Div: 3242.7234\n","Epoch[9/15], Step [270/469], Reconst Loss: 10063.3281, KL Div: 3208.0186\n","Epoch[9/15], Step [280/469], Reconst Loss: 10869.0293, KL Div: 3343.5869\n","Epoch[9/15], Step [290/469], Reconst Loss: 10607.9512, KL Div: 3172.1143\n","Epoch[9/15], Step [300/469], Reconst Loss: 10706.9082, KL Div: 3278.3369\n","Epoch[9/15], Step [310/469], Reconst Loss: 10387.4277, KL Div: 3247.9133\n","Epoch[9/15], Step [320/469], Reconst Loss: 10559.7939, KL Div: 3112.9216\n","Epoch[9/15], Step [330/469], Reconst Loss: 10671.3398, KL Div: 3229.5249\n","Epoch[9/15], Step [340/469], Reconst Loss: 10168.2939, KL Div: 3090.3174\n","Epoch[9/15], Step [350/469], Reconst Loss: 10326.1895, KL Div: 3057.5024\n","Epoch[9/15], Step [360/469], Reconst Loss: 10336.3262, KL Div: 3229.2927\n","Epoch[9/15], Step [370/469], Reconst Loss: 10381.3916, KL Div: 3214.4604\n","Epoch[9/15], Step [380/469], Reconst Loss: 10508.2783, KL Div: 3191.5811\n","Epoch[9/15], Step [390/469], Reconst Loss: 10468.2695, KL Div: 3213.1230\n","Epoch[9/15], Step [400/469], Reconst Loss: 10042.5859, KL Div: 3238.5383\n","Epoch[9/15], Step [410/469], Reconst Loss: 10405.7871, KL Div: 3237.0217\n","Epoch[9/15], Step [420/469], Reconst Loss: 10496.3535, KL Div: 3308.7800\n","Epoch[9/15], Step [430/469], Reconst Loss: 10508.6299, KL Div: 3236.8157\n","Epoch[9/15], Step [440/469], Reconst Loss: 10512.3477, KL Div: 3226.5222\n","Epoch[9/15], Step [450/469], Reconst Loss: 10619.2881, KL Div: 3311.4829\n","Epoch[9/15], Step [460/469], Reconst Loss: 10635.2656, KL Div: 3193.8394\n","Epoch[10/15], Step [10/469], Reconst Loss: 10719.2578, KL Div: 3241.3962\n","Epoch[10/15], Step [20/469], Reconst Loss: 10760.5410, KL Div: 3406.7749\n","Epoch[10/15], Step [30/469], Reconst Loss: 10369.0479, KL Div: 3172.3569\n","Epoch[10/15], Step [40/469], Reconst Loss: 10501.2676, KL Div: 3368.4062\n","Epoch[10/15], Step [50/469], Reconst Loss: 10072.7051, KL Div: 3285.2876\n","Epoch[10/15], Step [60/469], Reconst Loss: 10374.5898, KL Div: 3162.3188\n","Epoch[10/15], Step [70/469], Reconst Loss: 10555.7080, KL Div: 3257.3179\n","Epoch[10/15], Step [80/469], Reconst Loss: 9973.3652, KL Div: 3312.3845\n","Epoch[10/15], Step [90/469], Reconst Loss: 10191.4697, KL Div: 3144.5881\n","Epoch[10/15], Step [100/469], Reconst Loss: 10034.4648, KL Div: 3204.0327\n","Epoch[10/15], Step [110/469], Reconst Loss: 10307.3369, KL Div: 3279.7083\n","Epoch[10/15], Step [120/469], Reconst Loss: 10474.8867, KL Div: 3175.3030\n","Epoch[10/15], Step [130/469], Reconst Loss: 10805.7646, KL Div: 3328.4153\n","Epoch[10/15], Step [140/469], Reconst Loss: 9954.0762, KL Div: 3174.7725\n","Epoch[10/15], Step [150/469], Reconst Loss: 10493.9609, KL Div: 3232.6821\n","Epoch[10/15], Step [160/469], Reconst Loss: 10521.9316, KL Div: 3328.7937\n","Epoch[10/15], Step [170/469], Reconst Loss: 10808.8232, KL Div: 3286.8337\n","Epoch[10/15], Step [180/469], Reconst Loss: 10315.7354, KL Div: 3263.8442\n","Epoch[10/15], Step [190/469], Reconst Loss: 10841.9492, KL Div: 3344.3008\n","Epoch[10/15], Step [200/469], Reconst Loss: 10237.6328, KL Div: 3197.3027\n","Epoch[10/15], Step [210/469], Reconst Loss: 10516.3242, KL Div: 3197.5742\n","Epoch[10/15], Step [220/469], Reconst Loss: 10245.8848, KL Div: 3188.2964\n","Epoch[10/15], Step [230/469], Reconst Loss: 10302.9033, KL Div: 3269.7070\n","Epoch[10/15], Step [240/469], Reconst Loss: 11007.5146, KL Div: 3344.2910\n","Epoch[10/15], Step [250/469], Reconst Loss: 10254.7500, KL Div: 3212.8684\n","Epoch[10/15], Step [260/469], Reconst Loss: 10040.1367, KL Div: 3273.2637\n","Epoch[10/15], Step [270/469], Reconst Loss: 10537.6875, KL Div: 3208.7134\n","Epoch[10/15], Step [280/469], Reconst Loss: 10269.2549, KL Div: 3255.8120\n","Epoch[10/15], Step [290/469], Reconst Loss: 10158.1240, KL Div: 3212.4116\n","Epoch[10/15], Step [300/469], Reconst Loss: 10085.1553, KL Div: 3210.0688\n","Epoch[10/15], Step [310/469], Reconst Loss: 10483.7715, KL Div: 3235.0786\n","Epoch[10/15], Step [320/469], Reconst Loss: 10611.8359, KL Div: 3229.5208\n","Epoch[10/15], Step [330/469], Reconst Loss: 10832.7373, KL Div: 3298.6299\n","Epoch[10/15], Step [340/469], Reconst Loss: 10305.8203, KL Div: 3127.7102\n","Epoch[10/15], Step [350/469], Reconst Loss: 10414.0479, KL Div: 3367.8369\n","Epoch[10/15], Step [360/469], Reconst Loss: 10192.8828, KL Div: 3199.7100\n","Epoch[10/15], Step [370/469], Reconst Loss: 10203.4111, KL Div: 3250.4277\n","Epoch[10/15], Step [380/469], Reconst Loss: 10192.0938, KL Div: 3128.6450\n","Epoch[10/15], Step [390/469], Reconst Loss: 10622.3311, KL Div: 3353.5933\n","Epoch[10/15], Step [400/469], Reconst Loss: 10726.4570, KL Div: 3236.2080\n","Epoch[10/15], Step [410/469], Reconst Loss: 10564.5791, KL Div: 3265.2002\n","Epoch[10/15], Step [420/469], Reconst Loss: 10256.6748, KL Div: 3238.9343\n","Epoch[10/15], Step [430/469], Reconst Loss: 10588.4824, KL Div: 3266.3970\n","Epoch[10/15], Step [440/469], Reconst Loss: 10491.4482, KL Div: 3254.6284\n","Epoch[10/15], Step [450/469], Reconst Loss: 10307.4531, KL Div: 3260.7805\n","Epoch[10/15], Step [460/469], Reconst Loss: 10287.0996, KL Div: 3241.2771\n","Epoch[11/15], Step [10/469], Reconst Loss: 9982.9502, KL Div: 3132.2773\n","Epoch[11/15], Step [20/469], Reconst Loss: 10601.0000, KL Div: 3185.9402\n","Epoch[11/15], Step [30/469], Reconst Loss: 10032.6104, KL Div: 3136.8787\n","Epoch[11/15], Step [40/469], Reconst Loss: 10295.9746, KL Div: 3344.4502\n","Epoch[11/15], Step [50/469], Reconst Loss: 10115.8340, KL Div: 3275.6770\n","Epoch[11/15], Step [60/469], Reconst Loss: 9938.4834, KL Div: 3062.5571\n","Epoch[11/15], Step [70/469], Reconst Loss: 9792.0234, KL Div: 3185.8152\n","Epoch[11/15], Step [80/469], Reconst Loss: 10063.7373, KL Div: 3236.6472\n","Epoch[11/15], Step [90/469], Reconst Loss: 10621.5732, KL Div: 3343.2451\n","Epoch[11/15], Step [100/469], Reconst Loss: 10158.2686, KL Div: 3297.8149\n","Epoch[11/15], Step [110/469], Reconst Loss: 10201.7383, KL Div: 3182.0039\n","Epoch[11/15], Step [120/469], Reconst Loss: 10293.1201, KL Div: 3267.6941\n","Epoch[11/15], Step [130/469], Reconst Loss: 10503.7148, KL Div: 3179.3354\n","Epoch[11/15], Step [140/469], Reconst Loss: 10374.7061, KL Div: 3281.8987\n","Epoch[11/15], Step [150/469], Reconst Loss: 10206.8203, KL Div: 3266.5828\n","Epoch[11/15], Step [160/469], Reconst Loss: 10757.4658, KL Div: 3299.8572\n","Epoch[11/15], Step [170/469], Reconst Loss: 10266.9834, KL Div: 3228.3008\n","Epoch[11/15], Step [180/469], Reconst Loss: 10195.1660, KL Div: 3217.7246\n","Epoch[11/15], Step [190/469], Reconst Loss: 10364.7256, KL Div: 3236.5154\n","Epoch[11/15], Step [200/469], Reconst Loss: 10577.4902, KL Div: 3255.8164\n","Epoch[11/15], Step [210/469], Reconst Loss: 10750.1738, KL Div: 3230.8035\n","Epoch[11/15], Step [220/469], Reconst Loss: 10256.6211, KL Div: 3309.5557\n","Epoch[11/15], Step [230/469], Reconst Loss: 9994.8965, KL Div: 3165.4106\n","Epoch[11/15], Step [240/469], Reconst Loss: 10072.5361, KL Div: 3196.0923\n","Epoch[11/15], Step [250/469], Reconst Loss: 9886.8965, KL Div: 3195.0698\n","Epoch[11/15], Step [260/469], Reconst Loss: 10558.5205, KL Div: 3228.1987\n","Epoch[11/15], Step [270/469], Reconst Loss: 9988.3545, KL Div: 3180.6235\n","Epoch[11/15], Step [280/469], Reconst Loss: 10058.9043, KL Div: 3316.6685\n","Epoch[11/15], Step [290/469], Reconst Loss: 10352.4658, KL Div: 3237.6741\n","Epoch[11/15], Step [300/469], Reconst Loss: 10444.7432, KL Div: 3301.1895\n","Epoch[11/15], Step [310/469], Reconst Loss: 9976.1357, KL Div: 3222.1179\n","Epoch[11/15], Step [320/469], Reconst Loss: 10654.5498, KL Div: 3215.9346\n","Epoch[11/15], Step [330/469], Reconst Loss: 10487.0137, KL Div: 3227.6694\n","Epoch[11/15], Step [340/469], Reconst Loss: 10310.3643, KL Div: 3216.0947\n","Epoch[11/15], Step [350/469], Reconst Loss: 10722.9082, KL Div: 3293.0525\n","Epoch[11/15], Step [360/469], Reconst Loss: 10198.4639, KL Div: 3244.3740\n","Epoch[11/15], Step [370/469], Reconst Loss: 9957.0820, KL Div: 3277.6750\n","Epoch[11/15], Step [380/469], Reconst Loss: 10540.4512, KL Div: 3408.1384\n","Epoch[11/15], Step [390/469], Reconst Loss: 9768.9414, KL Div: 3134.6841\n","Epoch[11/15], Step [400/469], Reconst Loss: 10419.2686, KL Div: 3344.2817\n","Epoch[11/15], Step [410/469], Reconst Loss: 10343.5371, KL Div: 3218.0991\n","Epoch[11/15], Step [420/469], Reconst Loss: 10442.8691, KL Div: 3373.9272\n","Epoch[11/15], Step [430/469], Reconst Loss: 10439.0420, KL Div: 3199.5149\n","Epoch[11/15], Step [440/469], Reconst Loss: 10548.5928, KL Div: 3345.4580\n","Epoch[11/15], Step [450/469], Reconst Loss: 10003.2324, KL Div: 3192.1282\n","Epoch[11/15], Step [460/469], Reconst Loss: 10847.7002, KL Div: 3264.8450\n","Epoch[12/15], Step [10/469], Reconst Loss: 10012.9414, KL Div: 3237.2632\n","Epoch[12/15], Step [20/469], Reconst Loss: 10795.4033, KL Div: 3262.8896\n","Epoch[12/15], Step [30/469], Reconst Loss: 10000.0459, KL Div: 3271.6331\n","Epoch[12/15], Step [40/469], Reconst Loss: 10258.9736, KL Div: 3203.0959\n","Epoch[12/15], Step [50/469], Reconst Loss: 10631.6387, KL Div: 3150.3945\n","Epoch[12/15], Step [60/469], Reconst Loss: 10252.5752, KL Div: 3276.3416\n","Epoch[12/15], Step [70/469], Reconst Loss: 10325.0996, KL Div: 3278.4678\n","Epoch[12/15], Step [80/469], Reconst Loss: 10703.7383, KL Div: 3262.6829\n","Epoch[12/15], Step [90/469], Reconst Loss: 10617.9014, KL Div: 3242.3813\n","Epoch[12/15], Step [100/469], Reconst Loss: 10566.8633, KL Div: 3354.0479\n","Epoch[12/15], Step [110/469], Reconst Loss: 9980.4287, KL Div: 3239.1064\n","Epoch[12/15], Step [120/469], Reconst Loss: 10433.5586, KL Div: 3302.0371\n","Epoch[12/15], Step [130/469], Reconst Loss: 10320.4951, KL Div: 3234.8760\n","Epoch[12/15], Step [140/469], Reconst Loss: 9629.7119, KL Div: 3111.4150\n","Epoch[12/15], Step [150/469], Reconst Loss: 9840.8525, KL Div: 3239.5854\n","Epoch[12/15], Step [160/469], Reconst Loss: 10166.8213, KL Div: 3243.1880\n","Epoch[12/15], Step [170/469], Reconst Loss: 9852.0977, KL Div: 3177.8247\n","Epoch[12/15], Step [180/469], Reconst Loss: 10196.8643, KL Div: 3260.2556\n","Epoch[12/15], Step [190/469], Reconst Loss: 9966.2910, KL Div: 3140.6372\n","Epoch[12/15], Step [200/469], Reconst Loss: 10206.7158, KL Div: 3284.0317\n","Epoch[12/15], Step [210/469], Reconst Loss: 10134.8701, KL Div: 3175.1702\n","Epoch[12/15], Step [220/469], Reconst Loss: 10275.5352, KL Div: 3199.4783\n","Epoch[12/15], Step [230/469], Reconst Loss: 10323.5078, KL Div: 3273.2954\n","Epoch[12/15], Step [240/469], Reconst Loss: 10189.9619, KL Div: 3216.0664\n","Epoch[12/15], Step [250/469], Reconst Loss: 10294.9014, KL Div: 3295.2666\n","Epoch[12/15], Step [260/469], Reconst Loss: 9985.6211, KL Div: 3230.9250\n","Epoch[12/15], Step [270/469], Reconst Loss: 10248.4551, KL Div: 3221.8083\n","Epoch[12/15], Step [280/469], Reconst Loss: 10189.0889, KL Div: 3323.4929\n","Epoch[12/15], Step [290/469], Reconst Loss: 10560.8604, KL Div: 3279.1670\n","Epoch[12/15], Step [300/469], Reconst Loss: 10753.0645, KL Div: 3243.3628\n","Epoch[12/15], Step [310/469], Reconst Loss: 10652.2051, KL Div: 3233.9900\n","Epoch[12/15], Step [320/469], Reconst Loss: 10311.9912, KL Div: 3156.6665\n","Epoch[12/15], Step [330/469], Reconst Loss: 10386.2568, KL Div: 3228.5823\n","Epoch[12/15], Step [340/469], Reconst Loss: 10279.2197, KL Div: 3232.5322\n","Epoch[12/15], Step [350/469], Reconst Loss: 9819.8145, KL Div: 3166.5652\n","Epoch[12/15], Step [360/469], Reconst Loss: 10138.5625, KL Div: 3319.5955\n","Epoch[12/15], Step [370/469], Reconst Loss: 10286.7637, KL Div: 3201.9771\n","Epoch[12/15], Step [380/469], Reconst Loss: 10450.0898, KL Div: 3322.0813\n","Epoch[12/15], Step [390/469], Reconst Loss: 9961.0010, KL Div: 3306.1162\n","Epoch[12/15], Step [400/469], Reconst Loss: 10401.4980, KL Div: 3232.9626\n","Epoch[12/15], Step [410/469], Reconst Loss: 10217.0762, KL Div: 3266.5364\n","Epoch[12/15], Step [420/469], Reconst Loss: 10363.2568, KL Div: 3336.2380\n","Epoch[12/15], Step [430/469], Reconst Loss: 10976.0684, KL Div: 3352.6428\n","Epoch[12/15], Step [440/469], Reconst Loss: 10523.0518, KL Div: 3227.3052\n","Epoch[12/15], Step [450/469], Reconst Loss: 9884.6807, KL Div: 3207.2407\n","Epoch[12/15], Step [460/469], Reconst Loss: 10193.0713, KL Div: 3269.8142\n","Epoch[13/15], Step [10/469], Reconst Loss: 9999.4580, KL Div: 3264.5269\n","Epoch[13/15], Step [20/469], Reconst Loss: 10144.4326, KL Div: 3266.1724\n","Epoch[13/15], Step [30/469], Reconst Loss: 10540.4531, KL Div: 3314.9604\n","Epoch[13/15], Step [40/469], Reconst Loss: 10309.8896, KL Div: 3303.2197\n","Epoch[13/15], Step [50/469], Reconst Loss: 10395.8555, KL Div: 3235.3179\n","Epoch[13/15], Step [60/469], Reconst Loss: 10438.2109, KL Div: 3280.6477\n","Epoch[13/15], Step [70/469], Reconst Loss: 10026.3721, KL Div: 3232.2495\n","Epoch[13/15], Step [80/469], Reconst Loss: 11041.7852, KL Div: 3360.0515\n","Epoch[13/15], Step [90/469], Reconst Loss: 10420.9805, KL Div: 3374.6919\n","Epoch[13/15], Step [100/469], Reconst Loss: 10294.6045, KL Div: 3294.3538\n","Epoch[13/15], Step [110/469], Reconst Loss: 10438.4453, KL Div: 3255.3408\n","Epoch[13/15], Step [120/469], Reconst Loss: 10091.9619, KL Div: 3127.3921\n","Epoch[13/15], Step [130/469], Reconst Loss: 9937.9678, KL Div: 3221.4775\n","Epoch[13/15], Step [140/469], Reconst Loss: 10292.4092, KL Div: 3248.7539\n","Epoch[13/15], Step [150/469], Reconst Loss: 9915.2910, KL Div: 3131.1621\n","Epoch[13/15], Step [160/469], Reconst Loss: 10015.6201, KL Div: 3204.3936\n","Epoch[13/15], Step [170/469], Reconst Loss: 10213.0967, KL Div: 3293.2566\n","Epoch[13/15], Step [180/469], Reconst Loss: 9772.5928, KL Div: 3210.9155\n","Epoch[13/15], Step [190/469], Reconst Loss: 10204.4814, KL Div: 3218.0354\n","Epoch[13/15], Step [200/469], Reconst Loss: 9883.5156, KL Div: 3237.0254\n","Epoch[13/15], Step [210/469], Reconst Loss: 10221.5576, KL Div: 3317.4404\n","Epoch[13/15], Step [220/469], Reconst Loss: 10231.0459, KL Div: 3326.3591\n","Epoch[13/15], Step [230/469], Reconst Loss: 9811.1592, KL Div: 3136.3826\n","Epoch[13/15], Step [240/469], Reconst Loss: 9708.0527, KL Div: 3238.0188\n","Epoch[13/15], Step [250/469], Reconst Loss: 10403.5859, KL Div: 3333.4561\n","Epoch[13/15], Step [260/469], Reconst Loss: 10101.8252, KL Div: 3245.6414\n","Epoch[13/15], Step [270/469], Reconst Loss: 10396.9707, KL Div: 3298.7830\n","Epoch[13/15], Step [280/469], Reconst Loss: 10105.8516, KL Div: 3208.7178\n","Epoch[13/15], Step [290/469], Reconst Loss: 10011.2119, KL Div: 3253.4380\n","Epoch[13/15], Step [300/469], Reconst Loss: 10106.0947, KL Div: 3274.3618\n","Epoch[13/15], Step [310/469], Reconst Loss: 9958.8105, KL Div: 3209.5618\n","Epoch[13/15], Step [320/469], Reconst Loss: 10468.4316, KL Div: 3244.4575\n","Epoch[13/15], Step [330/469], Reconst Loss: 9898.7480, KL Div: 3282.8035\n","Epoch[13/15], Step [340/469], Reconst Loss: 10016.4121, KL Div: 3162.9829\n","Epoch[13/15], Step [350/469], Reconst Loss: 10002.5117, KL Div: 3164.8711\n","Epoch[13/15], Step [360/469], Reconst Loss: 10355.4805, KL Div: 3320.6453\n","Epoch[13/15], Step [370/469], Reconst Loss: 9841.2324, KL Div: 3172.5432\n","Epoch[13/15], Step [380/469], Reconst Loss: 10020.9678, KL Div: 3207.6646\n","Epoch[13/15], Step [390/469], Reconst Loss: 10770.7812, KL Div: 3362.1731\n","Epoch[13/15], Step [400/469], Reconst Loss: 10299.2500, KL Div: 3301.6709\n","Epoch[13/15], Step [410/469], Reconst Loss: 10376.2344, KL Div: 3315.9998\n","Epoch[13/15], Step [420/469], Reconst Loss: 10086.0625, KL Div: 3246.4004\n","Epoch[13/15], Step [430/469], Reconst Loss: 10370.7334, KL Div: 3285.7234\n","Epoch[13/15], Step [440/469], Reconst Loss: 9948.8438, KL Div: 3174.7981\n","Epoch[13/15], Step [450/469], Reconst Loss: 10400.6611, KL Div: 3291.3237\n","Epoch[13/15], Step [460/469], Reconst Loss: 10287.1318, KL Div: 3279.6821\n","Epoch[14/15], Step [10/469], Reconst Loss: 10650.9980, KL Div: 3360.7712\n","Epoch[14/15], Step [20/469], Reconst Loss: 10018.4814, KL Div: 3291.2729\n","Epoch[14/15], Step [30/469], Reconst Loss: 10176.0176, KL Div: 3244.2019\n","Epoch[14/15], Step [40/469], Reconst Loss: 10555.5312, KL Div: 3370.2019\n","Epoch[14/15], Step [50/469], Reconst Loss: 10801.8926, KL Div: 3389.1826\n","Epoch[14/15], Step [60/469], Reconst Loss: 9825.7998, KL Div: 3024.7817\n","Epoch[14/15], Step [70/469], Reconst Loss: 9832.9238, KL Div: 3308.9475\n","Epoch[14/15], Step [80/469], Reconst Loss: 10108.0625, KL Div: 3222.3870\n","Epoch[14/15], Step [90/469], Reconst Loss: 10209.2559, KL Div: 3267.3928\n","Epoch[14/15], Step [100/469], Reconst Loss: 10450.9814, KL Div: 3340.5354\n","Epoch[14/15], Step [110/469], Reconst Loss: 10792.7012, KL Div: 3291.3318\n","Epoch[14/15], Step [120/469], Reconst Loss: 10558.6738, KL Div: 3320.4219\n","Epoch[14/15], Step [130/469], Reconst Loss: 10244.8242, KL Div: 3299.5325\n","Epoch[14/15], Step [140/469], Reconst Loss: 10257.7930, KL Div: 3173.9790\n","Epoch[14/15], Step [150/469], Reconst Loss: 10321.7695, KL Div: 3377.6328\n","Epoch[14/15], Step [160/469], Reconst Loss: 9906.0078, KL Div: 3227.5073\n","Epoch[14/15], Step [170/469], Reconst Loss: 10333.2227, KL Div: 3151.9099\n","Epoch[14/15], Step [180/469], Reconst Loss: 10395.3760, KL Div: 3312.8613\n","Epoch[14/15], Step [190/469], Reconst Loss: 10146.5049, KL Div: 3241.3130\n","Epoch[14/15], Step [200/469], Reconst Loss: 10373.0957, KL Div: 3277.8833\n","Epoch[14/15], Step [210/469], Reconst Loss: 10372.7500, KL Div: 3264.7939\n","Epoch[14/15], Step [220/469], Reconst Loss: 10150.7588, KL Div: 3170.5024\n","Epoch[14/15], Step [230/469], Reconst Loss: 10145.5439, KL Div: 3275.1799\n","Epoch[14/15], Step [240/469], Reconst Loss: 9909.0986, KL Div: 3288.3286\n","Epoch[14/15], Step [250/469], Reconst Loss: 10042.6738, KL Div: 3164.3535\n","Epoch[14/15], Step [260/469], Reconst Loss: 10434.5967, KL Div: 3279.1035\n","Epoch[14/15], Step [270/469], Reconst Loss: 10238.5020, KL Div: 3250.8569\n","Epoch[14/15], Step [280/469], Reconst Loss: 10550.9785, KL Div: 3260.1763\n","Epoch[14/15], Step [290/469], Reconst Loss: 9981.2627, KL Div: 3148.8464\n","Epoch[14/15], Step [300/469], Reconst Loss: 10187.3389, KL Div: 3339.4868\n","Epoch[14/15], Step [310/469], Reconst Loss: 10371.4463, KL Div: 3257.8999\n","Epoch[14/15], Step [320/469], Reconst Loss: 10075.2861, KL Div: 3320.6646\n","Epoch[14/15], Step [330/469], Reconst Loss: 10504.5840, KL Div: 3256.7070\n","Epoch[14/15], Step [340/469], Reconst Loss: 10295.3203, KL Div: 3259.1890\n","Epoch[14/15], Step [350/469], Reconst Loss: 10266.7139, KL Div: 3222.1482\n","Epoch[14/15], Step [360/469], Reconst Loss: 10128.7637, KL Div: 3355.4783\n","Epoch[14/15], Step [370/469], Reconst Loss: 9900.8096, KL Div: 3257.0337\n","Epoch[14/15], Step [380/469], Reconst Loss: 10501.5000, KL Div: 3314.8687\n","Epoch[14/15], Step [390/469], Reconst Loss: 10406.1338, KL Div: 3219.0535\n","Epoch[14/15], Step [400/469], Reconst Loss: 9711.8047, KL Div: 3164.8521\n","Epoch[14/15], Step [410/469], Reconst Loss: 9869.8193, KL Div: 3290.2412\n","Epoch[14/15], Step [420/469], Reconst Loss: 10704.8398, KL Div: 3385.5422\n","Epoch[14/15], Step [430/469], Reconst Loss: 10190.4053, KL Div: 3276.3845\n","Epoch[14/15], Step [440/469], Reconst Loss: 10367.8086, KL Div: 3281.8892\n","Epoch[14/15], Step [450/469], Reconst Loss: 10012.2285, KL Div: 3248.7888\n","Epoch[14/15], Step [460/469], Reconst Loss: 10280.2988, KL Div: 3321.2751\n","Epoch[15/15], Step [10/469], Reconst Loss: 9826.4600, KL Div: 3277.2405\n","Epoch[15/15], Step [20/469], Reconst Loss: 9916.3760, KL Div: 3189.3079\n","Epoch[15/15], Step [30/469], Reconst Loss: 10228.4395, KL Div: 3291.5325\n","Epoch[15/15], Step [40/469], Reconst Loss: 9927.3242, KL Div: 3208.6743\n","Epoch[15/15], Step [50/469], Reconst Loss: 10087.6191, KL Div: 3203.9456\n","Epoch[15/15], Step [60/469], Reconst Loss: 9994.8770, KL Div: 3309.4150\n","Epoch[15/15], Step [70/469], Reconst Loss: 9906.1211, KL Div: 3230.7236\n","Epoch[15/15], Step [80/469], Reconst Loss: 10236.7158, KL Div: 3131.2822\n","Epoch[15/15], Step [90/469], Reconst Loss: 10089.9707, KL Div: 3296.6060\n","Epoch[15/15], Step [100/469], Reconst Loss: 9885.9111, KL Div: 3162.8794\n","Epoch[15/15], Step [110/469], Reconst Loss: 10094.9893, KL Div: 3257.9600\n","Epoch[15/15], Step [120/469], Reconst Loss: 10461.1328, KL Div: 3281.8591\n","Epoch[15/15], Step [130/469], Reconst Loss: 10350.5107, KL Div: 3266.6582\n","Epoch[15/15], Step [140/469], Reconst Loss: 10395.3262, KL Div: 3337.0339\n","Epoch[15/15], Step [150/469], Reconst Loss: 10109.1562, KL Div: 3210.4058\n","Epoch[15/15], Step [160/469], Reconst Loss: 10479.3350, KL Div: 3256.1538\n","Epoch[15/15], Step [170/469], Reconst Loss: 10003.8281, KL Div: 3165.9741\n","Epoch[15/15], Step [180/469], Reconst Loss: 9792.1582, KL Div: 3250.3918\n","Epoch[15/15], Step [190/469], Reconst Loss: 9964.1211, KL Div: 3172.4067\n","Epoch[15/15], Step [200/469], Reconst Loss: 10263.2969, KL Div: 3263.4048\n","Epoch[15/15], Step [210/469], Reconst Loss: 10091.7637, KL Div: 3354.6294\n","Epoch[15/15], Step [220/469], Reconst Loss: 10234.6611, KL Div: 3252.0190\n","Epoch[15/15], Step [230/469], Reconst Loss: 9739.7412, KL Div: 3264.1277\n","Epoch[15/15], Step [240/469], Reconst Loss: 10382.5000, KL Div: 3277.2346\n","Epoch[15/15], Step [250/469], Reconst Loss: 10364.1592, KL Div: 3156.0649\n","Epoch[15/15], Step [260/469], Reconst Loss: 9896.7490, KL Div: 3230.0039\n","Epoch[15/15], Step [270/469], Reconst Loss: 9793.1465, KL Div: 3204.4390\n","Epoch[15/15], Step [280/469], Reconst Loss: 9890.3135, KL Div: 3255.1140\n","Epoch[15/15], Step [290/469], Reconst Loss: 9889.7344, KL Div: 3202.7737\n","Epoch[15/15], Step [300/469], Reconst Loss: 10169.9512, KL Div: 3215.8450\n","Epoch[15/15], Step [310/469], Reconst Loss: 10371.0469, KL Div: 3260.9006\n","Epoch[15/15], Step [320/469], Reconst Loss: 10516.6650, KL Div: 3370.1667\n","Epoch[15/15], Step [330/469], Reconst Loss: 10221.6963, KL Div: 3144.4839\n","Epoch[15/15], Step [340/469], Reconst Loss: 10301.6787, KL Div: 3200.1326\n","Epoch[15/15], Step [350/469], Reconst Loss: 10246.9238, KL Div: 3191.4456\n","Epoch[15/15], Step [360/469], Reconst Loss: 10381.1387, KL Div: 3326.8323\n","Epoch[15/15], Step [370/469], Reconst Loss: 10452.1504, KL Div: 3298.7205\n","Epoch[15/15], Step [380/469], Reconst Loss: 10072.6484, KL Div: 3192.6343\n","Epoch[15/15], Step [390/469], Reconst Loss: 9734.7881, KL Div: 3231.4929\n","Epoch[15/15], Step [400/469], Reconst Loss: 10571.8311, KL Div: 3339.6514\n","Epoch[15/15], Step [410/469], Reconst Loss: 10108.7656, KL Div: 3275.4941\n","Epoch[15/15], Step [420/469], Reconst Loss: 9982.1582, KL Div: 3207.1450\n","Epoch[15/15], Step [430/469], Reconst Loss: 10286.8369, KL Div: 3385.7693\n","Epoch[15/15], Step [440/469], Reconst Loss: 10416.5547, KL Div: 3231.4189\n","Epoch[15/15], Step [450/469], Reconst Loss: 10645.8164, KL Div: 3268.1907\n","Epoch[15/15], Step [460/469], Reconst Loss: 10380.4805, KL Div: 3366.2092\n"],"name":"stdout"}]}]}