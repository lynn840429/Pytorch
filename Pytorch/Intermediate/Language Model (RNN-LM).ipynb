{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Language Model (RNN-LM).ipynb","provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"code","metadata":{"id":"XDVd038ihe8i","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":156},"outputId":"b1a47c80-4120-46c0-aef8-a016c5ef2692","executionInfo":{"status":"ok","timestamp":1574919612735,"user_tz":-480,"elapsed":11078,"user":{"displayName":"Lynn","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAA3iweeMBLyNHjfWr2cL84enkEcjNXptNrvoZJUw=s64","userId":"13191593999274530441"}}},"source":["!pip install keras"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: keras in /usr/local/lib/python3.6/dist-packages (2.2.5)\n","Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras) (1.3.2)\n","Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras) (1.12.0)\n","Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.6/dist-packages (from keras) (1.0.8)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras) (2.8.0)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras) (3.13)\n","Requirement already satisfied: keras-preprocessing>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from keras) (1.1.0)\n","Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras) (1.17.4)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"SYseVEu3D0ns","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":374},"outputId":"62df2481-cfda-4547-e216-3a9ca0641db2","executionInfo":{"status":"error","timestamp":1574920073303,"user_tz":-480,"elapsed":2105,"user":{"displayName":"Lynn","photoUrl":"https://lh3.googleusercontent.com/a-/AAuE7mAA3iweeMBLyNHjfWr2cL84enkEcjNXptNrvoZJUw=s64","userId":"13191593999274530441"}}},"source":["# Some part of the code was referenced from below.\n","# https://github.com/pytorch/examples/tree/master/word_language_model \n","import torch\n","import torch.nn as nn\n","import numpy as np\n","from torch.nn.utils import clip_grad_norm_\n","from data_utils import Dictionary, Corpus\n","\n","\n","# Device configuration\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","# Hyper-parameters\n","embed_size = 128\n","hidden_size = 1024\n","num_layers = 1\n","num_epochs = 5\n","num_samples = 1000     # number of words to be sampled\n","batch_size = 20\n","seq_length = 30\n","learning_rate = 0.002\n","\n","# Load \"Penn Treebank\" dataset\n","corpus = Corpus()\n","ids = corpus.get_data('data/train.txt', batch_size)\n","vocab_size = len(corpus.dictionary)\n","num_batches = ids.size(1) // seq_length\n","\n","\n","# RNN based language model\n","class RNNLM(nn.Module):\n","    def __init__(self, vocab_size, embed_size, hidden_size, num_layers):\n","        super(RNNLM, self).__init__()\n","        self.embed = nn.Embedding(vocab_size, embed_size)\n","        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, batch_first=True)\n","        self.linear = nn.Linear(hidden_size, vocab_size)\n","        \n","    def forward(self, x, h):\n","        # Embed word ids to vectors\n","        x = self.embed(x)\n","        \n","        # Forward propagate LSTM\n","        out, (h, c) = self.lstm(x, h)\n","        \n","        # Reshape output to (batch_size*sequence_length, hidden_size)\n","        out = out.reshape(out.size(0)*out.size(1), out.size(2))\n","        \n","        # Decode hidden states of all time steps\n","        out = self.linear(out)\n","        return out, (h, c)\n","\n","model = RNNLM(vocab_size, embed_size, hidden_size, num_layers).to(device)\n","\n","# Loss and optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n","\n","# Truncated backpropagation\n","def detach(states):\n","    return [state.detach() for state in states] \n","\n","# Train the model\n","for epoch in range(num_epochs):\n","    # Set initial hidden and cell states\n","    states = (torch.zeros(num_layers, batch_size, hidden_size).to(device),\n","              torch.zeros(num_layers, batch_size, hidden_size).to(device))\n","    \n","    for i in range(0, ids.size(1) - seq_length, seq_length):\n","        # Get mini-batch inputs and targets\n","        inputs = ids[:, i:i+seq_length].to(device)\n","        targets = ids[:, (i+1):(i+1)+seq_length].to(device)\n","        \n","        # Forward pass\n","        states = detach(states)\n","        outputs, states = model(inputs, states)\n","        loss = criterion(outputs, targets.reshape(-1))\n","        \n","        # Backward and optimize\n","        model.zero_grad()\n","        loss.backward()\n","        clip_grad_norm_(model.parameters(), 0.5)\n","        optimizer.step()\n","\n","        step = (i+1) // seq_length\n","        if step % 100 == 0:\n","            print ('Epoch [{}/{}], Step[{}/{}], Loss: {:.4f}, Perplexity: {:5.2f}'\n","                   .format(epoch+1, num_epochs, step, num_batches, loss.item(), np.exp(loss.item())))\n","\n","# Test the model\n","with torch.no_grad():\n","    with open('sample.txt', 'w') as f:\n","        # Set intial hidden ane cell states\n","        state = (torch.zeros(num_layers, 1, hidden_size).to(device),\n","                 torch.zeros(num_layers, 1, hidden_size).to(device))\n","\n","        # Select one word id randomly\n","        prob = torch.ones(vocab_size)\n","        input = torch.multinomial(prob, num_samples=1).unsqueeze(1).to(device)\n","\n","        for i in range(num_samples):\n","            # Forward propagate RNN \n","            output, state = model(input, state)\n","\n","            # Sample a word id\n","            prob = output.exp()\n","            word_id = torch.multinomial(prob, num_samples=1).item()\n","\n","            # Fill input with sampled word id for the next time step\n","            input.fill_(word_id)\n","\n","            # File write\n","            word = corpus.dictionary.idx2word[word_id]\n","            word = '\\n' if word == '<eos>' else word + ' '\n","            f.write(word)\n","\n","            if (i+1) % 100 == 0:\n","                print('Sampled [{}/{}] words and save to {}'.format(i+1, num_samples, 'sample.txt'))\n","\n","# Save the model checkpoints\n","torch.save(model.state_dict(), 'model.ckpt')"],"execution_count":1,"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-6af2252e561f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mclip_grad_norm_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mdata_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDictionary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCorpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'data_utils'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"]}]}]}